#@title #Choose a Model
model = 'mixtral-8x7b-instruct-v0.1.Q2_K' #@param ['mixtral-8x7b-instruct-v0.1.Q2_K', 'dolphin-2.6-mixtral-8x7b.Q2_K', 'mistral-7b-instruct-v0.2.Q8', 'dolphin-2.2.1-mistral-7b.Q8']

#@title #Runtime Infohttps://github.com/tarunminator/mistral_sandbox/settings
gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)
from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))
if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

#@title # Setup
%cd /content/
!rm -rf llama.cpp
!git clone --depth 1 https://github.com/ggerganov/llama.cpp.git
%cd llama.cpp
!make LLAMA_CUBLAS=1

if model == 'mixtral-8x7b-instruct-v0.1.Q2_K':
	!wget https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q2_K.gguf
elif model == 'dolphin-2.6-mixtral-8x7b.Q2_K':
	!wget https://huggingface.co/TheBloke/dolphin-2.6-mixtral-8x7b-GGUF/resolve/main/dolphin-2.6-mixtral-8x7b.Q2_K.gguf
elif model == 'mistral-7b-instruct-v0.2.Q8':
	!wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q8_0.gguf
elif model == 'dolphin-2.2.1-mistral-7b.Q8':
	!wget https://huggingface.co/TheBloke/dolphin-2.2.1-mistral-7B-GGUF/resolve/main/dolphin-2.2.1-mistral-7b.Q8_0.gguf


#@title # Connect using the following address when the server is up.
from google.colab.output import eval_js
print(eval_js("google.colab.kernel.proxyPort(12345)"))


#@title # inference
%cd /content/llama.cpp
if model == 'mixtral-8x7b-instruct-v0.1.Q2_K':
	!./server -m mixtral-8x7b-instruct-v0.1.Q2_K.gguf -ngl 27 -c 2048 --port 12345
elif model == 'dolphin-2.6-mixtral-8x7b.Q2_K':
	!./server -m dolphin-2.6-mixtral-8x7b.Q2_K.gguf -ngl 27 -c 2048 --port 12345
elif model == 'mistral-7b-instruct-v0.2.Q8':
	!./server -m mistral-7b-instruct-v0.2.Q8_0.gguf -ngl 35 -c 0 --port 12345
elif model == 'dolphin-2.2.1-mistral-7b.Q8':
	!./server -m dolphin-2.2.1-mistral-7b.Q8_0.gguf -ngl 35 -c 0 --port 12345


